console.log('üîç Daily scraper starting...');
console.log('üìÅ Current working directory:', process.cwd());
console.log('üìã Files in current directory:', require('fs').readdirSync('.'));

const { spawn } = require('child_process');
const path = require('path');
const fs = require('fs');

async function runScript(scriptName, args = []) {
    return new Promise((resolve, reject) => {
        console.log(`üöÄ Starting ${scriptName}...`);
        if (args.length > 0) console.log(`   Args: ${args.join(' ')}`);
        console.log(`üìÖ ${new Date().toISOString()}`);

        const child = spawn('node', [scriptName, ...args], {
            stdio: 'inherit',
            cwd: process.cwd()
        });

        console.log(`‚úÖ Spawn created for ${scriptName}`);

        child.on('close', (code) => {
            console.log(`üìä ${scriptName} process closed with code: ${code}`);
            if (code === 0) {
                console.log(`‚úÖ ${scriptName} completed successfully`);
                resolve(code);
            } else {
                console.log(`‚ùå ${scriptName} failed with exit code: ${code}`);
                reject(new Error(`${scriptName} failed with exit code: ${code}`));
            }
        });

        child.on('error', (error) => {
            console.log(`üí• Error running ${scriptName}:`, error.message);
            reject(error);
        });
    });
}

async function delay(seconds) {
    console.log(`‚è≥ Waiting ${seconds} seconds...`);
    return new Promise(resolve => setTimeout(resolve, seconds * 1000));
}

async function main() {
    console.log('üèãÔ∏è Daily Scraper & Database Import Started');
    console.log('==========================================');
    console.log(`üìç Working directory: ${process.cwd()}`);
    console.log(`üïê Start time: ${new Date().toLocaleString('en-US', { timeZone: 'America/New_York' })}`);

    try {
        // Step 1: Run meet scraper
        // Pass any command line arguments (like --date=2025-12) to the scraper
        const args = process.argv.slice(2);
        await runScript('scripts/production/meet_scraper.js', args);

        // Step 2: Wait 10 seconds
        await delay(10);

        // Step 3: Run address scraper (New Step)
        console.log('\nüìç Step 3: Running Meet Address Scraper...');
        // Pass the same args (e.g., --date=...) to the address scraper
        await runScript('scripts/production/meet-address-scraper.js', args);

        // Step 4: Run geocoder (New Step)
        console.log('\nüåç Step 4: Running Geocoder...');
        await runScript('scripts/geographic/geocode-and-import.js');

        // Step 5: Import to database
        await runScript('scripts/production/database-importer.js', args);

        // Step 6: Pipeline Handoff - Reimport & WSO Backfill
        const scrapedMeetsPath = 'output/scraped_meets.json';
        if (fs.existsSync(scrapedMeetsPath)) {
            console.log('\nüîÑ Checking for scraped meets to post-process...');
            const meetIds = JSON.parse(fs.readFileSync(scrapedMeetsPath, 'utf8'));

            if (meetIds && meetIds.length > 0) {
                console.log(`üéØ Found ${meetIds.length} meets to post-process: ${meetIds.join(', ')}`);
                const meetIdString = meetIds.join(',');

                // Step 6a: Run Reimport (Data Quality Check)
                console.log('\nüîç Step 6a: Running Data Quality Reimport (High Fidelity)...');
                // The user wants to FORCE this for all new meets to ensure we get the best data immediately
                // This uses the "Custom" importer under the hood which has the multi-tier verification
                await runScript('scripts/unified-scraper.js', [
                    '--mode=reimport',
                    `--meet-ids=${meetIdString}`,
                    '--force'
                ]);

                // Step 6b: Run WSO Backfill (Metadata Enrichment)
                console.log('\nüåç Step 6b: Running WSO Metadata Backfill...');
                // Run without restrictive flags to catch anything that needs metadata
                await runScript('scripts/unified-scraper.js', [
                    '--mode=wso',
                    `--meet-ids=${meetIdString}`
                ]);

            } else {
                console.log('‚ÑπÔ∏è Scraped meets file exists but is empty. No post-processing needed.');
            }
        } else {
            console.log('‚ÑπÔ∏è No scraped_meets.json found. Skipping post-processing.');
        }

        console.log('\nüéâ Daily scraping and import completed successfully!');
        console.log(`üïê End time: ${new Date().toLocaleString()}`);
        process.exit(0); // Exit cleanly so the process doesn't hang

    } catch (error) {
        console.log('\nüí• Daily pipeline failed:', error.message);
        console.log(`üïê Failed at: ${new Date().toLocaleString()}`);
        process.exit(1); // Exit with error code so GitHub Actions knows it failed
    }
}

// Run the main function
main();
